{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * rnn for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part, task prepared by Александр Панин, jheuristic@yandex-team.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# For students with low-RAM machines\n",
    " * This assignment can be accomplished with even the low-tier hardware (<= 4Gb RAM) \n",
    " * If that is the case, turn flag \"low_RAM_mode\" below to True\n",
    " * If you have around 8GB memory, it is unlikely that you will feel constrained by memory.\n",
    " * In case you are using a PC from last millenia, consider setting very_low_RAM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = True\n",
    "very_low_RAM = False  #If you have <3GB RAM, set BOTH to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # a lot of ram\n",
    "    df = pd.read_csv(\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #aroung 4GB ram\n",
    "    df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1204949, 13) 0.228222107326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000299</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Костюм Didriksons Boardman, размер 100, краги,...</td>\n",
       "      <td>Костюм Didriksons Boardman, в отличном состоян...</td>\n",
       "      <td>{\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000309</td>\n",
       "      <td>Недвижимость</td>\n",
       "      <td>Квартиры</td>\n",
       "      <td>1-к квартира, 44 м², 9/20 эт.</td>\n",
       "      <td>В кирпичном пан.-м доме, продается одноком.-ая...</td>\n",
       "      <td>{\"Тип объявления\":\"Продам\", \"Количество комнат...</td>\n",
       "      <td>2642020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000317</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Поездки на таможню, печать в паспорте</td>\n",
       "      <td>Поездки на таможню гражданам СНГ для пересечен...</td>\n",
       "      <td>{\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid      category                subcategory  \\\n",
       "0  10000010     Транспорт      Автомобили с пробегом   \n",
       "1  10000094   Личные вещи  Одежда, обувь, аксессуары   \n",
       "2  10000299   Личные вещи     Детская одежда и обувь   \n",
       "3  10000309  Недвижимость                   Квартиры   \n",
       "4  10000317        Услуги          Предложения услуг   \n",
       "\n",
       "                                               title  \\\n",
       "0                                  Toyota Sera, 1991   \n",
       "1                                   Костюм Steilmann   \n",
       "2  Костюм Didriksons Boardman, размер 100, краги,...   \n",
       "3                      1-к квартира, 44 м², 9/20 эт.   \n",
       "4              Поездки на таможню, печать в паспорте   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "2  Костюм Didriksons Boardman, в отличном состоян...   \n",
       "3  В кирпичном пан.-м доме, продается одноком.-ая...   \n",
       "4  Поездки на таможню гражданам СНГ для пересечен...   \n",
       "\n",
       "                                               attrs    price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...   150000        NaN   \n",
       "1  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...     1500        NaN   \n",
       "2  {\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...     3000        NaN   \n",
       "3  {\"Тип объявления\":\"Продам\", \"Количество комнат...  2642020        NaN   \n",
       "4  {\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...     1500        0.0   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           0           0         0         0.41  \n",
       "2           0           0           0         0         5.49  \n",
       "3           0           1           0         0        22.47  \n",
       "4           1           0           0         0         1.43  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (df.shape, df.is_blocked.mean())\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 0.228222107326\n",
      "Count: 1204949\n"
     ]
    }
   ],
   "source": [
    "print (\"Blocked ratio\",df.is_blocked.mean())\n",
    "print (\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    " * 250k banned out of 4kk\n",
    " * Let's just downsample random 250k legal samples to make further steps less computationally demanding\n",
    " * If you aim for high Kaggle score, consider a smarter approach to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio: 0.505833013138\n",
      "Count: 489284\n"
     ]
    }
   ],
   "source": [
    "#downsample\n",
    "\n",
    "df_bad = df[df[\"is_blocked\"] == 1].sample(frac=0.9)\n",
    "df_good = df[df[\"is_blocked\"] == 0].sample(frac=0.26)\n",
    "df = pd.concat([df_good, df_bad])\n",
    "\n",
    "print (\"Blocked ratio:\",df.is_blocked.mean())\n",
    "print (\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print (\"All tests passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#In case your RAM-o-meter is in the red\n",
    "if very_low_RAM:\n",
    "    data = data[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9d86249080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "\n",
    "_=plt.hist(list(token_counts.values()),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "\n",
    "min_count = 10\n",
    "tokens = [i for i, _ in token_counts.items() if token_counts[i] >= min_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 82453\n"
     ]
    }
   ],
   "source": [
    "print (\"# Tokens:\",len(token_to_id))\n",
    "if len(token_to_id) < 30000:\n",
    "    print (\"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\")\n",
    "if len(token_to_id) > 1000000:\n",
    "    print (\"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = list(map(lambda token: token_to_id.get(token,0), tokens))[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (489284, 15)\n",
      "ВАЗ 2106, 1995 -> [44070  9811 78076     0     0     0     0     0     0     0] ...\n",
      "Ford Focus, 2006 -> [ 8541 69707 45910     0     0     0     0     0     0     0] ...\n",
      "Очаровательные медвежата -> [42250 13467     0     0     0     0     0     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print (\"Размер матрицы:\",title_tokens.shape)\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print (title,'->', tokens[:10],'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "data_cat_subcat = df[[\"category\",\"subcategory\"]].values\n",
    "# print (data_cat_subcat)\n",
    "\n",
    "categories = [{\"category\": row[0], \"subcategory\": row[1]} for row in data_cat_subcat]\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Split into training and test set.\n",
    "\n",
    "#Easy: split randomly\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = train_test_split(title_tokens, \n",
    "                                                                                              desc_tokens,\n",
    "                                                                                              df_non_text,\n",
    "                                                                                              target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles: RNN\n",
    "* Separate input for description: RNN\n",
    "* Separate input for categorical features: обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal with binary_crossentropy\n",
    " * 2 softmax with categorical_crossentropy - essentially the same as previous one\n",
    " * 1 neuron without nonlinearity (lambda x: x) +  hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 978, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 828, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 573, in format\n",
      "    record.exc_text = self.formatException(record.exc_info)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 523, in formatException\n",
      "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
      "  File \"/usr/lib/python3.4/traceback.py\", line 169, in print_exception\n",
      "    for line in _format_exception_iter(etype, value, tb, limit, chain):\n",
      "  File \"/usr/lib/python3.4/traceback.py\", line 146, in _format_exception_iter\n",
      "    for value, tb in values:\n",
      "  File \"/usr/lib/python3.4/traceback.py\", line 125, in _iter_chain\n",
      "    context = exc.__context__\n",
      "AttributeError: 'NoneType' object has no attribute '__context__'\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/tornado/ioloop.py\", line 887, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-22-c618843c8194>\", line 2, in <module>\n",
      "    import lasagne\n",
      "  File \"<frozen importlib._bootstrap>\", line 2237, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 2226, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1200, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1129, in _exec\n",
      "  File \"<frozen importlib._bootstrap>\", line 1471, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 321, in _call_with_frames_removed\n",
      "  File \"/home/anata-m/.local/lib/python3.4/site-packages/lasagne/__init__.py\", line 12, in <module>\n",
      "    import theano\n",
      "  File \"<frozen importlib._bootstrap>\", line 2237, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 2226, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1200, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1129, in _exec\n",
      "  File \"<frozen importlib._bootstrap>\", line 1471, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 321, in _call_with_frames_removed\n",
      "  File \"/home/anata-m/.local/lib/python3.4/site-packages/theano/__init__.py\", line 134, in <module>\n",
      "    import theano.gpuarray\n",
      "  File \"<frozen importlib._bootstrap>\", line 2237, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 2226, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1200, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1129, in _exec\n",
      "  File \"<frozen importlib._bootstrap>\", line 1471, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 321, in _call_with_frames_removed\n",
      "  File \"/home/anata-m/.local/lib/python3.4/site-packages/theano/gpuarray/__init__.py\", line 207, in <module>\n",
      "    exc_info=True)\n",
      "Message: 'pygpu was configured but could not be imported or is too old (version 0.6 or higher required)'\n",
      "Arguments: ()\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "descr_nn = lasagne.layers.EmbeddingLayer(descr_inp, input_size=len(token_to_id)+1, output_size=128)\n",
    "descr_nnn = lasagne.layers.LSTMLayer(descr_nn, 8, nonlinearity=lasagne.nonlinearities.softmax, grad_clipping=0.15)\n",
    "descr_nnn = lasagne.layers.FlattenLayer(descr_nnn)\n",
    "\n",
    "# Titles\n",
    "title_nn = lasagne.layers.EmbeddingLayer(title_inp, input_size=len(token_to_id)+1, output_size=128)\n",
    "title_nnn = lasagne.layers.LSTMLayer(title_nn, 8, nonlinearity=lasagne.nonlinearities.softmax, grad_clipping=0.15)\n",
    "title_nnn = lasagne.layers.FlattenLayer(title_nnn)\n",
    "\n",
    "# Non-sequences\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp, 8, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "cat_nnn = lasagne.layers.FlattenLayer(cat_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nn = lasagne.layers.concat([descr_nnn, title_nnn, cat_nnn])\n",
    "\n",
    "nn = lasagne.layers.DenseLayer(nn, 34)\n",
    "nn = lasagne.layers.DropoutLayer(nn, p=0.25)\n",
    "nn = lasagne.layers.BatchNormLayer(nn)\n",
    "nn = lasagne.layers.DenseLayer(nn, 1, nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * delta is a tunable parameter: how far should a neuron be in the positive margin area for us to stop bothering about it\n",
    " * Function description may mention some +-1  limitations - this is not neccessary, at least as long as hinge loss has a __default__ flag `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(nn, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(nn)[:,0]\n",
    "\n",
    "#Hinge loss\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction, target_y, delta = 1., log_odds=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "updates = lasagne.updates.adam(loss, params=weights, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(nn, deterministic=True)[:,0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(det_prediction, target_y, delta = 1., log_odds=True).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anata-m/.local/lib/python3.4/site-packages/theano/tensor/basic.py:5130: UserWarning: flatten outdim parameter is deprecated, use ndim instead.\n",
      "  \"flatten outdim parameter is deprecated, use ndim instead.\")\n",
      "WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    }
   ],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump around 0.5 for several epochs\n",
    "\n",
    "* AUC is the most stable of all three metrics\n",
    "\n",
    "* Average Precision at top 2.5% (APatK) - is the least stable. If batch_size*minibatches_per_epoch < 10k, it behaves as a uniform random variable.\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 0.320271760562\n",
      "\tacc: 0.867544554455\n",
      "\tauc: 0.936548580404\n",
      "\tap@k: 0.98597744543\n",
      "Val:\n",
      "\tloss: 0.19754770414\n",
      "\tacc: 0.922118811881\n",
      "\tauc: 0.974387998446\n",
      "\tap@k: 0.996373860547\n",
      "Train:\n",
      "\tloss: 0.184071764719\n",
      "\tacc: 0.926712871287\n",
      "\tauc: 0.97250601272\n",
      "\tap@k: 0.988521510541\n",
      "Val:\n",
      "\tloss: 0.191062813753\n",
      "\tacc: 0.925069306931\n",
      "\tauc: 0.977696132979\n",
      "\tap@k: 0.999332057716\n",
      "Train:\n",
      "\tloss: 0.170930840643\n",
      "\tacc: 0.933762376238\n",
      "\tauc: 0.973670233341\n",
      "\tap@k: 0.986615950522\n",
      "Val:\n",
      "\tloss: 0.197333148876\n",
      "\tacc: 0.925227722772\n",
      "\tauc: 0.977128324782\n",
      "\tap@k: 0.99443416288\n",
      "Train:\n",
      "\tloss: 0.165558587467\n",
      "\tacc: 0.935207920792\n",
      "\tauc: 0.974647465112\n",
      "\tap@k: 0.991282247247\n",
      "Val:\n",
      "\tloss: 0.17662694486\n",
      "\tacc: 0.92798019802\n",
      "\tauc: 0.976638625882\n",
      "\tap@k: 0.996921289753\n",
      "Train:\n",
      "\tloss: 0.183059734712\n",
      "\tacc: 0.929722772277\n",
      "\tauc: 0.971633740073\n",
      "\tap@k: 0.995798037104\n",
      "Val:\n",
      "\tloss: 0.354439024145\n",
      "\tacc: 0.877267326733\n",
      "\tauc: 0.9767641647\n",
      "\tap@k: 0.997786650912\n",
      "Train:\n",
      "\tloss: 0.206610537924\n",
      "\tacc: 0.923069306931\n",
      "\tauc: 0.967922096681\n",
      "\tap@k: 0.991483940857\n",
      "Val:\n",
      "\tloss: 14.3997987416\n",
      "\tacc: 0.506\n",
      "\tauc: 0.975104375115\n",
      "\tap@k: 0.993978589977\n",
      "Train:\n",
      "\tloss: 0.18735312542\n",
      "\tacc: 0.929762376238\n",
      "\tauc: 0.970498295804\n",
      "\tap@k: 0.98327630178\n",
      "Val:\n",
      "\tloss: 0.178495078383\n",
      "\tacc: 0.927405940594\n",
      "\tauc: 0.97564250772\n",
      "\tap@k: 0.996065015852\n",
      "Train:\n",
      "\tloss: 0.18171567092\n",
      "\tacc: 0.932158415842\n",
      "\tauc: 0.972006828955\n",
      "\tap@k: 0.991501080222\n",
      "Val:\n",
      "\tloss: 1.79759074779\n",
      "\tacc: 0.830336633663\n",
      "\tauc: 0.971119639342\n",
      "\tap@k: 0.992573294726\n",
      "Train:\n",
      "\tloss: 0.173953439702\n",
      "\tacc: 0.933742574257\n",
      "\tauc: 0.972828138162\n",
      "\tap@k: 0.993668131228\n",
      "Val:\n",
      "\tloss: 0.187717538796\n",
      "\tacc: 0.921267326733\n",
      "\tauc: 0.972145812185\n",
      "\tap@k: 0.992077122268\n",
      "Train:\n",
      "\tloss: 0.157842680421\n",
      "\tacc: 0.937524752475\n",
      "\tauc: 0.976211022921\n",
      "\tap@k: 0.987890750343\n",
      "Val:\n",
      "\tloss: 0.189640721039\n",
      "\tacc: 0.929821782178\n",
      "\tauc: 0.974362597073\n",
      "\tap@k: 0.998268322742\n",
      "Train:\n",
      "\tloss: 0.152719442921\n",
      "\tacc: 0.939326732673\n",
      "\tauc: 0.97584514925\n",
      "\tap@k: 0.990676397322\n",
      "Val:\n",
      "\tloss: 0.183719437285\n",
      "\tacc: 0.92803960396\n",
      "\tauc: 0.975035915343\n",
      "\tap@k: 0.996077230038\n",
      "Train:\n",
      "\tloss: 0.148660305711\n",
      "\tacc: 0.941465346535\n",
      "\tauc: 0.977774088153\n",
      "\tap@k: 0.991740835984\n",
      "Val:\n",
      "\tloss: 0.182375278564\n",
      "\tacc: 0.928732673267\n",
      "\tauc: 0.976093314614\n",
      "\tap@k: 0.99704828019\n",
      "Train:\n",
      "\tloss: 0.158889082162\n",
      "\tacc: 0.941643564356\n",
      "\tauc: 0.975306747643\n",
      "\tap@k: 0.987143808158\n",
      "Val:\n",
      "\tloss: 0.215015592843\n",
      "\tacc: 0.923821782178\n",
      "\tauc: 0.976810160161\n",
      "\tap@k: 0.973587426307\n",
      "Train:\n",
      "\tloss: 0.153727103623\n",
      "\tacc: 0.941722772277\n",
      "\tauc: 0.975367100627\n",
      "\tap@k: 0.97319595959\n",
      "Val:\n",
      "\tloss: 0.201867750338\n",
      "\tacc: 0.923663366337\n",
      "\tauc: 0.977471199813\n",
      "\tap@k: 0.997425273657\n",
      "Train:\n",
      "\tloss: 0.156945320378\n",
      "\tacc: 0.939841584158\n",
      "\tauc: 0.97625764356\n",
      "\tap@k: 0.993629140502\n",
      "Val:\n",
      "\tloss: 0.173507595004\n",
      "\tacc: 0.929425742574\n",
      "\tauc: 0.973973776752\n",
      "\tap@k: 0.977374346239\n",
      "Train:\n",
      "\tloss: 0.154069581411\n",
      "\tacc: 0.94299009901\n",
      "\tauc: 0.976734121357\n",
      "\tap@k: 0.993986175253\n",
      "Val:\n",
      "\tloss: 0.179432452262\n",
      "\tacc: 0.924376237624\n",
      "\tauc: 0.974639372682\n",
      "\tap@k: 0.990603958863\n",
      "Train:\n",
      "\tloss: 0.157295169743\n",
      "\tacc: 0.941287128713\n",
      "\tauc: 0.976710124479\n",
      "\tap@k: 0.988438329649\n",
      "Val:\n",
      "\tloss: 0.210643460676\n",
      "\tacc: 0.924613861386\n",
      "\tauc: 0.976632629294\n",
      "\tap@k: 0.996667477791\n",
      "Train:\n",
      "\tloss: 0.155028015517\n",
      "\tacc: 0.940495049505\n",
      "\tauc: 0.976109887021\n",
      "\tap@k: 0.992212154552\n",
      "Val:\n",
      "\tloss: 0.175680143421\n",
      "\tacc: 0.929128712871\n",
      "\tauc: 0.976266827803\n",
      "\tap@k: 0.996791266131\n",
      "Train:\n",
      "\tloss: 0.156584385983\n",
      "\tacc: 0.942237623762\n",
      "\tauc: 0.976443327965\n",
      "\tap@k: 0.986207010866\n",
      "Val:\n",
      "\tloss: 0.210701395102\n",
      "\tacc: 0.92801980198\n",
      "\tauc: 0.973424024239\n",
      "\tap@k: 0.960275084036\n",
      "Train:\n",
      "\tloss: 0.160687387281\n",
      "\tacc: 0.940851485149\n",
      "\tauc: 0.977181789633\n",
      "\tap@k: 0.993000626096\n",
      "Val:\n",
      "\tloss: 0.183071163834\n",
      "\tacc: 0.927287128713\n",
      "\tauc: 0.975381516461\n",
      "\tap@k: 0.994340206\n",
      "Train:\n",
      "\tloss: 0.157904929726\n",
      "\tacc: 0.94003960396\n",
      "\tauc: 0.975545370959\n",
      "\tap@k: 0.989364839804\n",
      "Val:\n",
      "\tloss: 0.212744744379\n",
      "\tacc: 0.925306930693\n",
      "\tauc: 0.973601657412\n",
      "\tap@k: 0.986795623214\n",
      "Train:\n",
      "\tloss: 0.160591099724\n",
      "\tacc: 0.938653465347\n",
      "\tauc: 0.975872241097\n",
      "\tap@k: 0.994234025103\n",
      "Val:\n",
      "\tloss: 0.182240496286\n",
      "\tacc: 0.925782178218\n",
      "\tauc: 0.974378988217\n",
      "\tap@k: 0.991008730728\n",
      "Train:\n",
      "\tloss: 0.152422507738\n",
      "\tacc: 0.940792079208\n",
      "\tauc: 0.97762851183\n",
      "\tap@k: 0.996974901974\n",
      "Val:\n",
      "\tloss: 0.183369988996\n",
      "\tacc: 0.926336633663\n",
      "\tauc: 0.976841999129\n",
      "\tap@k: 0.998745761797\n",
      "Train:\n",
      "\tloss: 0.161185375375\n",
      "\tacc: 0.94001980198\n",
      "\tauc: 0.975191317496\n",
      "\tap@k: 0.993523973634\n",
      "Val:\n",
      "\tloss: 0.184758221953\n",
      "\tacc: 0.923188118812\n",
      "\tauc: 0.97309627637\n",
      "\tap@k: 0.993241811856\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-7fc08dd215d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mminibatches_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_probas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_desc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_title\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_cat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mb_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anata-m/.local/lib/python3.4/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anata-m/.local/lib/python3.4/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0mallow_gc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_gc\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_gc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0m\u001b[1;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m    963\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 500\n",
    "minibatches_per_epoch = 100\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr.as_matrix(),target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print (\"Train:\")\n",
    "    print ('\\tloss:',b_loss/b_c)\n",
    "    print ('\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print ('\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print ('\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_tr.as_matrix(),target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print (\"Val:\")\n",
    "    print ('\\tloss:',b_loss/b_c)\n",
    "    print ('\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print ('\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print ('\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ничего не меняется после 10 эпох, надоело ждать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_tr.as_matrix(),target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print (\"Scores:\")\n",
    "print ('\\tloss:',b_loss/b_c)\n",
    "print ('\\tacc:',final_accuracy)\n",
    "print ('\\tauc:',final_auc)\n",
    "print ('\\tap@k:',final_apatk)\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
