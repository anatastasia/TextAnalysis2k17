{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * rnn for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part, task prepared by Александр Панин, jheuristic@yandex-team.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# For students with low-RAM machines\n",
    " * This assignment can be accomplished with even the low-tier hardware (<= 4Gb RAM) \n",
    " * If that is the case, turn flag \"low_RAM_mode\" below to True\n",
    " * If you have around 8GB memory, it is unlikely that you will feel constrained by memory.\n",
    " * In case you are using a PC from last millenia, consider setting very_low_RAM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = True\n",
    "very_low_RAM = False  #If you have <3GB RAM, set BOTH to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # a lot of ram\n",
    "    df = pd.read_csv(\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #aroung 4GB ram\n",
    "    df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1204949, 13) 0.228222107326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000299</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Костюм Didriksons Boardman, размер 100, краги,...</td>\n",
       "      <td>Костюм Didriksons Boardman, в отличном состоян...</td>\n",
       "      <td>{\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000309</td>\n",
       "      <td>Недвижимость</td>\n",
       "      <td>Квартиры</td>\n",
       "      <td>1-к квартира, 44 м², 9/20 эт.</td>\n",
       "      <td>В кирпичном пан.-м доме, продается одноком.-ая...</td>\n",
       "      <td>{\"Тип объявления\":\"Продам\", \"Количество комнат...</td>\n",
       "      <td>2642020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000317</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Поездки на таможню, печать в паспорте</td>\n",
       "      <td>Поездки на таможню гражданам СНГ для пересечен...</td>\n",
       "      <td>{\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid      category                subcategory  \\\n",
       "0  10000010     Транспорт      Автомобили с пробегом   \n",
       "1  10000094   Личные вещи  Одежда, обувь, аксессуары   \n",
       "2  10000299   Личные вещи     Детская одежда и обувь   \n",
       "3  10000309  Недвижимость                   Квартиры   \n",
       "4  10000317        Услуги          Предложения услуг   \n",
       "\n",
       "                                               title  \\\n",
       "0                                  Toyota Sera, 1991   \n",
       "1                                   Костюм Steilmann   \n",
       "2  Костюм Didriksons Boardman, размер 100, краги,...   \n",
       "3                      1-к квартира, 44 м², 9/20 эт.   \n",
       "4              Поездки на таможню, печать в паспорте   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "2  Костюм Didriksons Boardman, в отличном состоян...   \n",
       "3  В кирпичном пан.-м доме, продается одноком.-ая...   \n",
       "4  Поездки на таможню гражданам СНГ для пересечен...   \n",
       "\n",
       "                                               attrs    price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...   150000        NaN   \n",
       "1  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...     1500        NaN   \n",
       "2  {\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...     3000        NaN   \n",
       "3  {\"Тип объявления\":\"Продам\", \"Количество комнат...  2642020        NaN   \n",
       "4  {\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...     1500        0.0   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           0           0         0         0.41  \n",
       "2           0           0           0         0         5.49  \n",
       "3           0           1           0         0        22.47  \n",
       "4           1           0           0         0         1.43  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (df.shape, df.is_blocked.mean())\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 0.228222107326\n",
      "Count: 1204949\n"
     ]
    }
   ],
   "source": [
    "print (\"Blocked ratio\",df.is_blocked.mean())\n",
    "print (\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    " * 250k banned out of 4kk\n",
    " * Let's just downsample random 250k legal samples to make further steps less computationally demanding\n",
    " * If you aim for high Kaggle score, consider a smarter approach to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio: 0.505833013138\n",
      "Count: 489284\n"
     ]
    }
   ],
   "source": [
    "#downsample\n",
    "\n",
    "df_bad = df[df[\"is_blocked\"] == 1].sample(frac=0.9)\n",
    "df_good = df[df[\"is_blocked\"] == 0].sample(frac=0.26)\n",
    "df = pd.concat([df_good, df_bad])\n",
    "\n",
    "print (\"Blocked ratio:\",df.is_blocked.mean())\n",
    "print (\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print (\"All tests passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#In case your RAM-o-meter is in the red\n",
    "if very_low_RAM:\n",
    "    data = data[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc9360948d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "\n",
    "_=plt.hist(list(token_counts.values()),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "\n",
    "min_count = 10\n",
    "tokens = [i for i, _ in token_counts.items() if token_counts[i] >= min_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 82463\n"
     ]
    }
   ],
   "source": [
    "print (\"# Tokens:\",len(token_to_id))\n",
    "if len(token_to_id) < 30000:\n",
    "    print (\"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\")\n",
    "if len(token_to_id) > 1000000:\n",
    "    print (\"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = list(map(lambda token: token_to_id.get(token,0), tokens))[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (489284, 15)\n",
      "Обучение игре на гитаре -> [15140 64530 17057 27548     0     0     0     0     0     0] ...\n",
      "Комната 18 м² в 1-к, 3 эт. -> [23708 46441 34368   390 38848 69906 33489 81123     0     0] ...\n",
      "2-к квартира, 46 м², 3/5 эт. -> [40280 69906  7402 45933 34368 33489 58562 81123     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print (\"Размер матрицы:\",title_tokens.shape)\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print (title,'->', tokens[:10],'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "data_cat_subcat = df[[\"category\",\"subcategory\"]].values\n",
    "# print (data_cat_subcat)\n",
    "\n",
    "categories = [{\"category\": row[0], \"subcategory\": row[1]} for row in data_cat_subcat]\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Split into training and test set.\n",
    "\n",
    "#Easy: split randomly\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = train_test_split(title_tokens, \n",
    "                                                                                              desc_tokens,\n",
    "                                                                                              df_non_text,\n",
    "                                                                                              target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles: RNN\n",
    "* Separate input for description: RNN\n",
    "* Separate input for categorical features: обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal with binary_crossentropy\n",
    " * 2 softmax with categorical_crossentropy - essentially the same as previous one\n",
    " * 1 neuron without nonlinearity (lambda x: x) +  hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 978, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 828, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 573, in format\n",
      "    record.exc_text = self.formatException(record.exc_info)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 523, in formatException\n",
      "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
      "  File \"/usr/lib/python3.4/traceback.py\", line 169, in print_exception\n",
      "    for line in _format_exception_iter(etype, value, tb, limit, chain):\n",
      "  File \"/usr/lib/python3.4/traceback.py\", line 146, in _format_exception_iter\n",
      "    for value, tb in values:\n",
      "  File \"/usr/lib/python3.4/traceback.py\", line 125, in _iter_chain\n",
      "    context = exc.__context__\n",
      "AttributeError: 'NoneType' object has no attribute '__context__'\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/tornado/ioloop.py\", line 887, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-22-c618843c8194>\", line 2, in <module>\n",
      "    import lasagne\n",
      "  File \"<frozen importlib._bootstrap>\", line 2237, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 2226, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1200, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1129, in _exec\n",
      "  File \"<frozen importlib._bootstrap>\", line 1471, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 321, in _call_with_frames_removed\n",
      "  File \"/home/anata-m/.local/lib/python3.4/site-packages/lasagne/__init__.py\", line 12, in <module>\n",
      "    import theano\n",
      "  File \"<frozen importlib._bootstrap>\", line 2237, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 2226, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1200, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1129, in _exec\n",
      "  File \"<frozen importlib._bootstrap>\", line 1471, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 321, in _call_with_frames_removed\n",
      "  File \"/home/anata-m/.local/lib/python3.4/site-packages/theano/__init__.py\", line 134, in <module>\n",
      "    import theano.gpuarray\n",
      "  File \"<frozen importlib._bootstrap>\", line 2237, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 2226, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1200, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1129, in _exec\n",
      "  File \"<frozen importlib._bootstrap>\", line 1471, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 321, in _call_with_frames_removed\n",
      "  File \"/home/anata-m/.local/lib/python3.4/site-packages/theano/gpuarray/__init__.py\", line 207, in <module>\n",
      "    exc_info=True)\n",
      "Message: 'pygpu was configured but could not be imported or is too old (version 0.6 or higher required)'\n",
      "Arguments: ()\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "descr_nn = lasagne.layers.EmbeddingLayer(descr_inp, input_size=len(token_to_id)+1, output_size=128)\n",
    "descr_nnn = lasagne.layers.RecurrentLayer(descr_nn, 3, nonlinearity=lasagne.nonlinearities.linear, grad_clipping=0.15)\n",
    "descr_nnn = lasagne.layers.DenseLayer(descr_nnn, 3, nonlinearity=lasagne.nonlinearities.linear)\n",
    "\n",
    "# Titles\n",
    "title_nn = lasagne.layers.EmbeddingLayer(title_inp, input_size=len(token_to_id)+1, output_size=128)\n",
    "title_nnn = lasagne.layers.RecurrentLayer(title_nn, 3, nonlinearity=lasagne.nonlinearities.linear, grad_clipping=0.15)\n",
    "title_nnn = lasagne.layers.DenseLayer(title_nnn, 3, nonlinearity=lasagne.nonlinearities.linear)\n",
    "\n",
    "# Non-sequences\n",
    "cat_nnn = lasagne.layers.DenseLayer(cat_inp, 3, nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nn = lasagne.layers.concat([descr_nnn, title_nnn, cat_nnn])\n",
    "\n",
    "nn = lasagne.layers.DenseLayer(nn, 34)\n",
    "nn = lasagne.layers.DropoutLayer(nn, p=0.15)\n",
    "nn = lasagne.layers.DenseLayer(nn, 1, nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * delta is a tunable parameter: how far should a neuron be in the positive margin area for us to stop bothering about it\n",
    " * Function description may mention some +-1  limitations - this is not neccessary, at least as long as hinge loss has a __default__ flag `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(nn, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(nn)[:,0]\n",
    "\n",
    "#Hinge loss\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction, target_y, delta = 0.5, log_odds=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "updates = lasagne.updates.adam(loss, params=weights, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(nn, deterministic=True)[:,0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(det_prediction, target_y, delta = 0.5, log_odds=True).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anata-m/.local/lib/python3.4/site-packages/theano/tensor/basic.py:5130: UserWarning: flatten outdim parameter is deprecated, use ndim instead.\n",
      "  \"flatten outdim parameter is deprecated, use ndim instead.\")\n",
      "WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    }
   ],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump around 0.5 for several epochs\n",
    "\n",
    "* AUC is the most stable of all three metrics\n",
    "\n",
    "* Average Precision at top 2.5% (APatK) - is the least stable. If batch_size*minibatches_per_epoch < 10k, it behaves as a uniform random variable.\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 11117.9357628\n",
      "\tacc: 0.562181818182\n",
      "\tauc: 0.597347449262\n",
      "\tap@k: 0.107599642013\n",
      "Val:\n",
      "\tloss: 19513.7638518\n",
      "\tacc: 0.490181818182\n",
      "\tauc: 0.514337172886\n",
      "\tap@k: 0.596790801403\n",
      "Train:\n",
      "\tloss: 5745.58525925\n",
      "\tacc: 0.637272727273\n",
      "\tauc: 0.66882598749\n",
      "\tap@k: 0.0958884440611\n",
      "Val:\n",
      "\tloss: 3228.41684549\n",
      "\tacc: 0.502363636364\n",
      "\tauc: 0.524930780156\n",
      "\tap@k: 0.795584254992\n",
      "Train:\n",
      "\tloss: 723.143233124\n",
      "\tacc: 0.588545454545\n",
      "\tauc: 0.648333758511\n",
      "\tap@k: 0.0685304902535\n",
      "Val:\n",
      "\tloss: 1644.38987847\n",
      "\tacc: 0.538363636364\n",
      "\tauc: 0.580991351952\n",
      "\tap@k: 0.923721110386\n",
      "Train:\n",
      "\tloss: 475.450236896\n",
      "\tacc: 0.618727272727\n",
      "\tauc: 0.647862381695\n",
      "\tap@k: 0.139020770745\n",
      "Val:\n",
      "\tloss: 1481.52477252\n",
      "\tacc: 0.655454545455\n",
      "\tauc: 0.666072858797\n",
      "\tap@k: 0.975395474608\n",
      "Train:\n",
      "\tloss: 486.303323116\n",
      "\tacc: 0.726545454545\n",
      "\tauc: 0.78519574571\n",
      "\tap@k: 0.0952459190344\n",
      "Val:\n",
      "\tloss: 799.68914357\n",
      "\tacc: 0.746181818182\n",
      "\tauc: 0.781625631711\n",
      "\tap@k: 0.985897963436\n",
      "Train:\n",
      "\tloss: 774.10123382\n",
      "\tacc: 0.759090909091\n",
      "\tauc: 0.807618188452\n",
      "\tap@k: 0.0657133241361\n",
      "Val:\n",
      "\tloss: 1021.88687262\n",
      "\tacc: 0.790181818182\n",
      "\tauc: 0.825340513522\n",
      "\tap@k: 0.954517859717\n",
      "Train:\n",
      "\tloss: 445.179388188\n",
      "\tacc: 0.844727272727\n",
      "\tauc: 0.908442502483\n",
      "\tap@k: 0.404963889197\n",
      "Val:\n",
      "\tloss: 1688.44311387\n",
      "\tacc: 0.811272727273\n",
      "\tauc: 0.848310258335\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 294.690726823\n",
      "\tacc: 0.853818181818\n",
      "\tauc: 0.922833613356\n",
      "\tap@k: 0.493357577977\n",
      "Val:\n",
      "\tloss: 1676.23333832\n",
      "\tacc: 0.825272727273\n",
      "\tauc: 0.879973981655\n",
      "\tap@k: 0.992091297705\n",
      "Train:\n",
      "\tloss: 403.779254537\n",
      "\tacc: 0.854727272727\n",
      "\tauc: 0.929582518754\n",
      "\tap@k: 0.805569091424\n",
      "Val:\n",
      "\tloss: 2008.21183893\n",
      "\tacc: 0.862363636364\n",
      "\tauc: 0.903540713624\n",
      "\tap@k: 0.916025864556\n",
      "Train:\n",
      "\tloss: 1719.71358041\n",
      "\tacc: 0.857636363636\n",
      "\tauc: 0.920713332599\n",
      "\tap@k: 0.538786784348\n",
      "Val:\n",
      "\tloss: 506.127410507\n",
      "\tacc: 0.893636363636\n",
      "\tauc: 0.950647610068\n",
      "\tap@k: 0.842062272991\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 500\n",
    "minibatches_per_epoch = 10\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr.as_matrix(),target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print (\"Train:\")\n",
    "    print ('\\tloss:',b_loss/b_c)\n",
    "    print ('\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print ('\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print ('\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_tr.as_matrix(),target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print (\"Val:\")\n",
    "    print ('\\tloss:',b_loss/b_c)\n",
    "    print ('\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print ('\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print ('\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \n"
     ]
    }
   ],
   "source": [
    "print (\"If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "\tloss: 544.008422484\n",
      "\tacc: 0.894262295082\n",
      "\tauc: 0.951873141897\n",
      "\tap@k: 0.880501949498\n",
      "\n",
      "AUC:\n",
      "\tСойдёт, хотя можно ещё поднажать (ok)\n",
      "\n",
      "Accuracy:\n",
      "Надо бы подтянуть. (not ok)\n",
      "\n",
      "Average precision at K:\n",
      "\tНадо бы поднажать (not ok)\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_tr.as_matrix(),target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print (\"Scores:\")\n",
    "print ('\\tloss:',b_loss/b_c)\n",
    "print ('\\tacc:',final_accuracy)\n",
    "print ('\\tauc:',final_auc)\n",
    "print ('\\tap@k:',final_apatk)\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Main task\n",
    "\n",
    "* https://goo.gl/forms/eJwIeAbjxzVuo6vn1\n",
    "* Feel like Le'Cun:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (test sample size * 0.025) > 0.99\n",
    " * And perhaps even farther\n",
    "\n",
    "* Casual mode\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (test sample size * 0.025) > 0.92\n",
    "\n",
    "* Remember the training, Luke\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * If you have background in texts, there may be a way to improve tokenizer, add some lemmatization, etc etc.\n",
    " * In case you know how not to shoot yourself in the foot with RNNs, they too may be of some use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
