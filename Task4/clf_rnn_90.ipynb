{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * rnn for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part, task prepared by Александр Панин, jheuristic@yandex-team.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# For students with low-RAM machines\n",
    " * This assignment can be accomplished with even the low-tier hardware (<= 4Gb RAM) \n",
    " * If that is the case, turn flag \"low_RAM_mode\" below to True\n",
    " * If you have around 8GB memory, it is unlikely that you will feel constrained by memory.\n",
    " * In case you are using a PC from last millenia, consider setting very_low_RAM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = True\n",
    "very_low_RAM = False  #If you have <3GB RAM, set BOTH to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # a lot of ram\n",
    "    df = pd.read_csv(\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #aroung 4GB ram\n",
    "    df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1204949, 13) 0.228222107326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000299</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Костюм Didriksons Boardman, размер 100, краги,...</td>\n",
       "      <td>Костюм Didriksons Boardman, в отличном состоян...</td>\n",
       "      <td>{\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000309</td>\n",
       "      <td>Недвижимость</td>\n",
       "      <td>Квартиры</td>\n",
       "      <td>1-к квартира, 44 м², 9/20 эт.</td>\n",
       "      <td>В кирпичном пан.-м доме, продается одноком.-ая...</td>\n",
       "      <td>{\"Тип объявления\":\"Продам\", \"Количество комнат...</td>\n",
       "      <td>2642020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000317</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Поездки на таможню, печать в паспорте</td>\n",
       "      <td>Поездки на таможню гражданам СНГ для пересечен...</td>\n",
       "      <td>{\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid      category                subcategory  \\\n",
       "0  10000010     Транспорт      Автомобили с пробегом   \n",
       "1  10000094   Личные вещи  Одежда, обувь, аксессуары   \n",
       "2  10000299   Личные вещи     Детская одежда и обувь   \n",
       "3  10000309  Недвижимость                   Квартиры   \n",
       "4  10000317        Услуги          Предложения услуг   \n",
       "\n",
       "                                               title  \\\n",
       "0                                  Toyota Sera, 1991   \n",
       "1                                   Костюм Steilmann   \n",
       "2  Костюм Didriksons Boardman, размер 100, краги,...   \n",
       "3                      1-к квартира, 44 м², 9/20 эт.   \n",
       "4              Поездки на таможню, печать в паспорте   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "2  Костюм Didriksons Boardman, в отличном состоян...   \n",
       "3  В кирпичном пан.-м доме, продается одноком.-ая...   \n",
       "4  Поездки на таможню гражданам СНГ для пересечен...   \n",
       "\n",
       "                                               attrs    price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...   150000        NaN   \n",
       "1  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...     1500        NaN   \n",
       "2  {\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...     3000        NaN   \n",
       "3  {\"Тип объявления\":\"Продам\", \"Количество комнат...  2642020        NaN   \n",
       "4  {\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...     1500        0.0   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           0           0         0         0.41  \n",
       "2           0           0           0         0         5.49  \n",
       "3           0           1           0         0        22.47  \n",
       "4           1           0           0         0         1.43  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (df.shape, df.is_blocked.mean())\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 0.228222107326\n",
      "Count: 1204949\n"
     ]
    }
   ],
   "source": [
    "print (\"Blocked ratio\",df.is_blocked.mean())\n",
    "print (\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    " * 250k banned out of 4kk\n",
    " * Let's just downsample random 250k legal samples to make further steps less computationally demanding\n",
    " * If you aim for high Kaggle score, consider a smarter approach to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio: 0.505833013138\n",
      "Count: 489284\n"
     ]
    }
   ],
   "source": [
    "#downsample\n",
    "\n",
    "df_bad = df[df[\"is_blocked\"] == 1].sample(frac=0.9)\n",
    "df_good = df[df[\"is_blocked\"] == 0].sample(frac=0.26)\n",
    "df = pd.concat([df_good, df_bad])\n",
    "\n",
    "print (\"Blocked ratio:\",df.is_blocked.mean())\n",
    "print (\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print (\"All tests passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#In case your RAM-o-meter is in the red\n",
    "if very_low_RAM:\n",
    "    data = data[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f16ddc2e9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "\n",
    "_=plt.hist(list(token_counts.values()),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "\n",
    "min_count = 10\n",
    "tokens = [i for i, _ in token_counts.items() if token_counts[i] >= min_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 82450\n"
     ]
    }
   ],
   "source": [
    "print (\"# Tokens:\",len(token_to_id))\n",
    "if len(token_to_id) < 30000:\n",
    "    print (\"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\")\n",
    "if len(token_to_id) > 1000000:\n",
    "    print (\"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = list(map(lambda token: token_to_id.get(token,0), tokens))[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (489284, 15)\n",
      "Андройд -> [51145     0     0     0     0     0     0     0     0     0] ...\n",
      "Туфли -> [36197     0     0     0     0     0     0     0     0     0] ...\n",
      "Дом 90 м² на участке 4 сот. -> [26935 42145 23675 19343 49778 47486 45831     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print (\"Размер матрицы:\",title_tokens.shape)\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print (title,'->', tokens[:10],'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "data_cat_subcat = df[[\"category\",\"subcategory\"]].values\n",
    "# print (data_cat_subcat)\n",
    "\n",
    "categories = [{\"category\": row[0], \"subcategory\": row[1]} for row in data_cat_subcat]\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Split into training and test set.\n",
    "\n",
    "#Easy: split randomly\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = train_test_split(title_tokens, \n",
    "                                                                                              desc_tokens,\n",
    "                                                                                              df_non_text,\n",
    "                                                                                              target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles: RNN\n",
    "* Separate input for description: RNN\n",
    "* Separate input for categorical features: обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal with binary_crossentropy\n",
    " * 2 softmax with categorical_crossentropy - essentially the same as previous one\n",
    " * 1 neuron without nonlinearity (lambda x: x) +  hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#libraries\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "descr_nn = lasagne.layers.EmbeddingLayer(descr_inp, input_size=len(token_to_id)+1, output_size=128)\n",
    "descr_nnn = lasagne.layers.LSTMLayer(descr_nn, 8, nonlinearity=lasagne.nonlinearities.softmax, grad_clipping=0.15)\n",
    "descr_nnn = lasagne.layers.DenseLayer(descr_nn, 8, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "# Titles\n",
    "title_nn = lasagne.layers.EmbeddingLayer(title_inp, input_size=len(token_to_id)+1, output_size=128)\n",
    "title_nnn = lasagne.layers.LSTMLayer(title_nn, 8, nonlinearity=lasagne.nonlinearities.softmax, grad_clipping=0.15)\n",
    "title_nnn = lasagne.layers.DenseLayer(title_nn, 8, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "# Non-sequences\n",
    "cat_nnn = lasagne.layers.DenseLayer(cat_inp, 8, nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nn = lasagne.layers.concat([descr_nnn, title_nnn, cat_nnn])\n",
    "\n",
    "nn = lasagne.layers.DenseLayer(nn, 34)\n",
    "nn = lasagne.layers.DropoutLayer(nn, p=0.25)\n",
    "nn = lasagne.layers.DenseLayer(nn, 1, nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * delta is a tunable parameter: how far should a neuron be in the positive margin area for us to stop bothering about it\n",
    " * Function description may mention some +-1  limitations - this is not neccessary, at least as long as hinge loss has a __default__ flag `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(nn, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(nn)[:,0]\n",
    "\n",
    "#Hinge loss\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction, target_y, delta = 1., log_odds=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "updates = lasagne.updates.momentum(loss, params=weights, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(nn, deterministic=True)[:,0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(det_prediction, target_y, delta = 1., log_odds=True).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump around 0.5 for several epochs\n",
    "\n",
    "* AUC is the most stable of all three metrics\n",
    "\n",
    "* Average Precision at top 2.5% (APatK) - is the least stable. If batch_size*minibatches_per_epoch < 10k, it behaves as a uniform random variable.\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 0.976416645119\n",
      "\tacc: 0.557727272727\n",
      "\tauc: 0.572894263566\n",
      "\tap@k: 0.6770661373\n",
      "Val:\n",
      "\tloss: 0.978187542815\n",
      "\tacc: 0.524090909091\n",
      "\tauc: 0.603799956358\n",
      "\tap@k: 0.648684316823\n",
      "Train:\n",
      "\tloss: 0.880150552802\n",
      "\tacc: 0.615909090909\n",
      "\tauc: 0.602438365244\n",
      "\tap@k: 0.687539761844\n",
      "Val:\n",
      "\tloss: 1.04038216984\n",
      "\tacc: 0.48\n",
      "\tauc: 0.586787155554\n",
      "\tap@k: 0.606808847965\n",
      "Train:\n",
      "\tloss: 0.857854635431\n",
      "\tacc: 0.600909090909\n",
      "\tauc: 0.609370039683\n",
      "\tap@k: 0.36861615351\n",
      "Val:\n",
      "\tloss: 1.06694575759\n",
      "\tacc: 0.505454545455\n",
      "\tauc: 0.594735241219\n",
      "\tap@k: 0.545243061869\n",
      "Train:\n",
      "\tloss: 0.996581498307\n",
      "\tacc: 0.508181818182\n",
      "\tauc: 0.519293251639\n",
      "\tap@k: 0.307392183472\n",
      "Val:\n",
      "\tloss: 1.03082735934\n",
      "\tacc: 0.481363636364\n",
      "\tauc: 0.388523643177\n",
      "\tap@k: 0.27368765422\n",
      "Train:\n",
      "\tloss: 0.970212014452\n",
      "\tacc: 0.540909090909\n",
      "\tauc: 0.575119897723\n",
      "\tap@k: 0.792597285086\n",
      "Val:\n",
      "\tloss: 1.00930386222\n",
      "\tacc: 0.486363636364\n",
      "\tauc: 0.381846716849\n",
      "\tap@k: 0.613838615616\n",
      "Train:\n",
      "\tloss: 0.889988989407\n",
      "\tacc: 0.579545454545\n",
      "\tauc: 0.613200594556\n",
      "\tap@k: 0.76129195561\n",
      "Val:\n",
      "\tloss: 0.993899993788\n",
      "\tacc: 0.509545454545\n",
      "\tauc: 0.458551298034\n",
      "\tap@k: 0.562558134786\n",
      "Train:\n",
      "\tloss: 0.871854210157\n",
      "\tacc: 0.591363636364\n",
      "\tauc: 0.596108958421\n",
      "\tap@k: 0.789833275584\n",
      "Val:\n",
      "\tloss: 1.00758103799\n",
      "\tacc: 0.495\n",
      "\tauc: 0.447369573752\n",
      "\tap@k: 0.562768461588\n",
      "Train:\n",
      "\tloss: 0.835489619049\n",
      "\tacc: 0.604545454545\n",
      "\tauc: 0.613883792049\n",
      "\tap@k: 0.774858848828\n",
      "Val:\n",
      "\tloss: 0.991324030432\n",
      "\tacc: 0.506818181818\n",
      "\tauc: 0.5662004924\n",
      "\tap@k: 0.569053721217\n",
      "Train:\n",
      "\tloss: 0.860745344365\n",
      "\tacc: 0.59\n",
      "\tauc: 0.586561475193\n",
      "\tap@k: 0.688877959876\n",
      "Val:\n",
      "\tloss: 0.998394576636\n",
      "\tacc: 0.495\n",
      "\tauc: 0.608102251417\n",
      "\tap@k: 0.605835549344\n",
      "Train:\n",
      "\tloss: 0.816712926044\n",
      "\tacc: 0.610454545455\n",
      "\tauc: 0.628545369148\n",
      "\tap@k: 0.824300855939\n",
      "Val:\n",
      "\tloss: 1.01755713506\n",
      "\tacc: 0.483181818182\n",
      "\tauc: 0.583788896841\n",
      "\tap@k: 0.535213178634\n",
      "Train:\n",
      "\tloss: 0.858340385204\n",
      "\tacc: 0.583636363636\n",
      "\tauc: 0.601208251896\n",
      "\tap@k: 0.865615860858\n",
      "Val:\n",
      "\tloss: 1.01250300516\n",
      "\tacc: 0.491818181818\n",
      "\tauc: 0.591642802537\n",
      "\tap@k: 0.550316467929\n",
      "Train:\n",
      "\tloss: 0.828138481135\n",
      "\tacc: 0.604545454545\n",
      "\tauc: 0.628502179446\n",
      "\tap@k: 0.970875254518\n",
      "Val:\n",
      "\tloss: 1.00137270377\n",
      "\tacc: 0.495\n",
      "\tauc: 0.604915289412\n",
      "\tap@k: 0.475013677584\n",
      "Train:\n",
      "\tloss: 0.850513888381\n",
      "\tacc: 0.593181818182\n",
      "\tauc: 0.604551044494\n",
      "\tap@k: 0.95616864312\n",
      "Val:\n",
      "\tloss: 1.0224568555\n",
      "\tacc: 0.485\n",
      "\tauc: 0.584267466541\n",
      "\tap@k: 0.601234714641\n",
      "Train:\n",
      "\tloss: 0.854771643586\n",
      "\tacc: 0.590454545455\n",
      "\tauc: 0.602655120318\n",
      "\tap@k: 0.907525189063\n",
      "Val:\n",
      "\tloss: 0.986775057451\n",
      "\tacc: 0.502272727273\n",
      "\tauc: 0.613316547352\n",
      "\tap@k: 0.661791746101\n",
      "Train:\n",
      "\tloss: 0.836187210483\n",
      "\tacc: 0.611363636364\n",
      "\tauc: 0.612151166684\n",
      "\tap@k: 0.904068892184\n",
      "Val:\n",
      "\tloss: 1.01915898215\n",
      "\tacc: 0.484090909091\n",
      "\tauc: 0.596270954952\n",
      "\tap@k: 0.633222661343\n",
      "Train:\n",
      "\tloss: 0.831781998243\n",
      "\tacc: 0.603181818182\n",
      "\tauc: 0.631027108088\n",
      "\tap@k: 0.837086225546\n",
      "Val:\n",
      "\tloss: 0.99045616624\n",
      "\tacc: 0.500454545455\n",
      "\tauc: 0.593140901037\n",
      "\tap@k: 0.611248739748\n",
      "Train:\n",
      "\tloss: 0.850849385119\n",
      "\tacc: 0.592272727273\n",
      "\tauc: 0.615769772257\n",
      "\tap@k: 0.859163215698\n",
      "Val:\n",
      "\tloss: 1.00566154829\n",
      "\tacc: 0.479090909091\n",
      "\tauc: 0.617275713661\n",
      "\tap@k: 0.581798498725\n",
      "Train:\n",
      "\tloss: 0.807865622003\n",
      "\tacc: 0.613181818182\n",
      "\tauc: 0.655372148785\n",
      "\tap@k: 0.864153963017\n",
      "Val:\n",
      "\tloss: 0.985957515565\n",
      "\tacc: 0.495\n",
      "\tauc: 0.597527059282\n",
      "\tap@k: 0.669254421743\n",
      "Train:\n",
      "\tloss: 0.803323994983\n",
      "\tacc: 0.613181818182\n",
      "\tauc: 0.670443227195\n",
      "\tap@k: 0.914788408461\n",
      "Val:\n",
      "\tloss: 0.954325422672\n",
      "\tacc: 0.498636363636\n",
      "\tauc: 0.604760141624\n",
      "\tap@k: 0.600818673181\n",
      "Train:\n",
      "\tloss: 0.797287390984\n",
      "\tacc: 0.609545454545\n",
      "\tauc: 0.686101797451\n",
      "\tap@k: 0.865459695718\n",
      "Val:\n",
      "\tloss: 0.934053260413\n",
      "\tacc: 0.487272727273\n",
      "\tauc: 0.606617954365\n",
      "\tap@k: 0.672816786862\n",
      "Train:\n",
      "\tloss: 0.807719047327\n",
      "\tacc: 0.603181818182\n",
      "\tauc: 0.678077872575\n",
      "\tap@k: 0.907610951977\n",
      "Val:\n",
      "\tloss: 0.915229548785\n",
      "\tacc: 0.501363636364\n",
      "\tauc: 0.613215859759\n",
      "\tap@k: 0.797706793012\n",
      "Train:\n",
      "\tloss: 0.791176658124\n",
      "\tacc: 0.625909090909\n",
      "\tauc: 0.687448399676\n",
      "\tap@k: 0.945086187455\n",
      "Val:\n",
      "\tloss: 0.86334514177\n",
      "\tacc: 0.588636363636\n",
      "\tauc: 0.62535836042\n",
      "\tap@k: 0.735544095464\n",
      "Train:\n",
      "\tloss: 0.761025005335\n",
      "\tacc: 0.630454545455\n",
      "\tauc: 0.721298753462\n",
      "\tap@k: 0.909688775977\n",
      "Val:\n",
      "\tloss: 0.848269807344\n",
      "\tacc: 0.594090909091\n",
      "\tauc: 0.641730438142\n",
      "\tap@k: 0.715551386863\n",
      "Train:\n",
      "\tloss: 0.753877361674\n",
      "\tacc: 0.636363636364\n",
      "\tauc: 0.701119475965\n",
      "\tap@k: 0.945527621575\n",
      "Val:\n",
      "\tloss: 0.805500010889\n",
      "\tacc: 0.608181818182\n",
      "\tauc: 0.632690080986\n",
      "\tap@k: 0.57111465953\n",
      "Train:\n",
      "\tloss: 0.745312442468\n",
      "\tacc: 0.633636363636\n",
      "\tauc: 0.720296087999\n",
      "\tap@k: 0.881667186693\n",
      "Val:\n",
      "\tloss: 0.803157376606\n",
      "\tacc: 0.605454545455\n",
      "\tauc: 0.663983800314\n",
      "\tap@k: 0.73697857412\n",
      "Train:\n",
      "\tloss: 0.711926684258\n",
      "\tacc: 0.67\n",
      "\tauc: 0.736602202296\n",
      "\tap@k: 0.85404579449\n",
      "Val:\n",
      "\tloss: 0.809308427681\n",
      "\tacc: 0.615454545455\n",
      "\tauc: 0.653798777983\n",
      "\tap@k: 0.761317042725\n",
      "Train:\n",
      "\tloss: 0.712669028017\n",
      "\tacc: 0.659545454545\n",
      "\tauc: 0.728682539315\n",
      "\tap@k: 0.942209637648\n",
      "Val:\n",
      "\tloss: 0.753063663678\n",
      "\tacc: 0.676818181818\n",
      "\tauc: 0.694640163544\n",
      "\tap@k: 0.637939181354\n",
      "Train:\n",
      "\tloss: 0.689817234535\n",
      "\tacc: 0.683181818182\n",
      "\tauc: 0.745352761713\n",
      "\tap@k: 0.950940027639\n",
      "Val:\n",
      "\tloss: 0.757996872365\n",
      "\tacc: 0.624090909091\n",
      "\tauc: 0.699576935285\n",
      "\tap@k: 0.775430870334\n",
      "Train:\n",
      "\tloss: 0.624509573105\n",
      "\tacc: 0.705\n",
      "\tauc: 0.782549242274\n",
      "\tap@k: 0.942898024631\n",
      "Val:\n",
      "\tloss: 0.693476096365\n",
      "\tacc: 0.665\n",
      "\tauc: 0.707727244524\n",
      "\tap@k: 0.67580217272\n",
      "Train:\n",
      "\tloss: 0.64347502787\n",
      "\tacc: 0.698181818182\n",
      "\tauc: 0.775002377261\n",
      "\tap@k: 0.845242628884\n",
      "Val:\n",
      "\tloss: 0.681122436794\n",
      "\tacc: 0.662272727273\n",
      "\tauc: 0.758659877231\n",
      "\tap@k: 0.622510381096\n",
      "Train:\n",
      "\tloss: 0.620397762873\n",
      "\tacc: 0.709545454545\n",
      "\tauc: 0.784135251323\n",
      "\tap@k: 0.881551094943\n",
      "Val:\n",
      "\tloss: 0.630766337893\n",
      "\tacc: 0.695909090909\n",
      "\tauc: 0.783389077181\n",
      "\tap@k: 0.71730602187\n",
      "Train:\n",
      "\tloss: 0.516594386236\n",
      "\tacc: 0.759090909091\n",
      "\tauc: 0.851382605738\n",
      "\tap@k: 0.989385766964\n",
      "Val:\n",
      "\tloss: 0.57851700138\n",
      "\tacc: 0.734090909091\n",
      "\tauc: 0.806540748253\n",
      "\tap@k: 0.764750943061\n",
      "Train:\n",
      "\tloss: 0.471967962438\n",
      "\tacc: 0.788181818182\n",
      "\tauc: 0.873702715559\n",
      "\tap@k: 0.994054887217\n",
      "Val:\n",
      "\tloss: 0.511626733569\n",
      "\tacc: 0.785909090909\n",
      "\tauc: 0.868330793313\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.477647539174\n",
      "\tacc: 0.779545454545\n",
      "\tauc: 0.866533597818\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.507589021569\n",
      "\tacc: 0.767272727273\n",
      "\tauc: 0.869017748819\n",
      "\tap@k: 0.963953353358\n",
      "Train:\n",
      "\tloss: 0.445883199031\n",
      "\tacc: 0.798181818182\n",
      "\tauc: 0.884893421135\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.456092476465\n",
      "\tacc: 0.797272727273\n",
      "\tauc: 0.886733213001\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.417542403273\n",
      "\tacc: 0.816363636364\n",
      "\tauc: 0.901148153169\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.417995760929\n",
      "\tacc: 0.813636363636\n",
      "\tauc: 0.901613595591\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.42323123069\n",
      "\tacc: 0.806818181818\n",
      "\tauc: 0.892862934184\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.443973672715\n",
      "\tacc: 0.791363636364\n",
      "\tauc: 0.915140609508\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.384424505207\n",
      "\tacc: 0.829545454545\n",
      "\tauc: 0.905048919441\n",
      "\tap@k: 0.995736723903\n",
      "Val:\n",
      "\tloss: 0.391349501799\n",
      "\tacc: 0.827272727273\n",
      "\tauc: 0.895900834479\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.356708864136\n",
      "\tacc: 0.841818181818\n",
      "\tauc: 0.923783783784\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.400917915729\n",
      "\tacc: 0.829545454545\n",
      "\tauc: 0.909018780724\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.368757004142\n",
      "\tacc: 0.843636363636\n",
      "\tauc: 0.91932536214\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.351277442561\n",
      "\tacc: 0.845454545455\n",
      "\tauc: 0.926581345218\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.364776299515\n",
      "\tacc: 0.845454545455\n",
      "\tauc: 0.896960850456\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.325409433571\n",
      "\tacc: 0.862272727273\n",
      "\tauc: 0.935205953142\n",
      "\tap@k: 0.991702001147\n",
      "Train:\n",
      "\tloss: 0.336889993603\n",
      "\tacc: 0.851818181818\n",
      "\tauc: 0.924441506929\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.335246771425\n",
      "\tacc: 0.861818181818\n",
      "\tauc: 0.931653308469\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.341339428127\n",
      "\tacc: 0.861363636364\n",
      "\tauc: 0.928001927542\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.367656109306\n",
      "\tacc: 0.846818181818\n",
      "\tauc: 0.918620620089\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.386519109715\n",
      "\tacc: 0.822727272727\n",
      "\tauc: 0.892303243805\n",
      "\tap@k: 0.993608458646\n",
      "Val:\n",
      "\tloss: 0.329979170154\n",
      "\tacc: 0.865454545455\n",
      "\tauc: 0.937439989282\n",
      "\tap@k: 0.999681122449\n",
      "Train:\n",
      "\tloss: 0.3397558296\n",
      "\tacc: 0.855454545455\n",
      "\tauc: 0.925024731425\n",
      "\tap@k: 0.984336109934\n",
      "Val:\n",
      "\tloss: 0.386985299669\n",
      "\tacc: 0.836818181818\n",
      "\tauc: 0.909539609811\n",
      "\tap@k: 0.979072913712\n",
      "Train:\n",
      "\tloss: 0.364375220293\n",
      "\tacc: 0.843181818182\n",
      "\tauc: 0.921454917067\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.347168581404\n",
      "\tacc: 0.844090909091\n",
      "\tauc: 0.938467535775\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.333496320627\n",
      "\tacc: 0.863181818182\n",
      "\tauc: 0.934530525338\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.370303017497\n",
      "\tacc: 0.84\n",
      "\tauc: 0.915972210735\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.309510564614\n",
      "\tacc: 0.866818181818\n",
      "\tauc: 0.932224977271\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.314909417846\n",
      "\tacc: 0.866818181818\n",
      "\tauc: 0.937007958656\n",
      "\tap@k: 0.976906422115\n",
      "Train:\n",
      "\tloss: 0.301868787814\n",
      "\tacc: 0.871818181818\n",
      "\tauc: 0.939068795059\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.339365121831\n",
      "\tacc: 0.845\n",
      "\tauc: 0.932316601518\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.283063652461\n",
      "\tacc: 0.875454545455\n",
      "\tauc: 0.943617225577\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.306302504269\n",
      "\tacc: 0.858636363636\n",
      "\tauc: 0.944562824006\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.317925981243\n",
      "\tacc: 0.864090909091\n",
      "\tauc: 0.931457679577\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.28965553094\n",
      "\tacc: 0.882272727273\n",
      "\tauc: 0.943957037669\n",
      "\tap@k: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 200\n",
    "minibatches_per_epoch = 10\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr.as_matrix(),target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print (\"Train:\")\n",
    "    print ('\\tloss:',b_loss/b_c)\n",
    "    print ('\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print ('\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print ('\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_tr.as_matrix(),target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print (\"Val:\")\n",
    "    print ('\\tloss:',b_loss/b_c)\n",
    "    print ('\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print ('\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print ('\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print (\"If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_tr.as_matrix(),target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print (\"Scores:\")\n",
    "print ('\\tloss:',b_loss/b_c)\n",
    "print ('\\tacc:',final_accuracy)\n",
    "print ('\\tauc:',final_auc)\n",
    "print ('\\tap@k:',final_apatk)\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Main task\n",
    "\n",
    "* https://goo.gl/forms/eJwIeAbjxzVuo6vn1\n",
    "* Feel like Le'Cun:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (test sample size * 0.025) > 0.99\n",
    " * And perhaps even farther\n",
    "\n",
    "* Casual mode\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (test sample size * 0.025) > 0.92\n",
    "\n",
    "* Remember the training, Luke\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * If you have background in texts, there may be a way to improve tokenizer, add some lemmatization, etc etc.\n",
    " * In case you know how not to shoot yourself in the foot with RNNs, they too may be of some use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
